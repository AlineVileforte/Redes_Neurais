{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMHHekECj+YXuk7lWNb2co"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["BIBLIOTECAS"],"metadata":{"id":"OWjjVwWA0MZo"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"pOjNB6t10OLA","executionInfo":{"status":"ok","timestamp":1752259983232,"user_tz":180,"elapsed":3092,"user":{"displayName":"Ecv Asl","userId":"09710885291478391939"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Funções de Ativação e Suas Derivadas"],"metadata":{"id":"ct7NTX6C0QNf"}},{"cell_type":"code","source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return (x > 0).astype(float)\n","\n","def tanh(x):\n","    return np.tanh(x)\n","\n","def tanh_derivative(x):\n","    return 1 - (x ** 2)\n","\n","def linear(x): # Função de ativação linear (identidade)\n","    return x\n","\n","def linear_derivative(x):\n","    return np.ones_like(x)\n","\n","# Dicionário para mapear strings para funções\n","ACTIVATION_FUNCTIONS = {\n","    'sigmoid': {'func': sigmoid, 'derivative': sigmoid_derivative},\n","    'relu': {'func': relu, 'derivative': relu_derivative},\n","    'tanh': {'func': tanh, 'derivative': tanh_derivative},\n","    'linear': {'func': linear, 'derivative': linear_derivative}\n","}"],"metadata":{"id":"5vJcgD-A0SP3","executionInfo":{"status":"ok","timestamp":1752259983242,"user_tz":180,"elapsed":12,"user":{"displayName":"Ecv Asl","userId":"09710885291478391939"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Implementação da Classe MLP"],"metadata":{"id":"DGbA5aNy0ZQu"}},{"cell_type":"code","source":["class MyMLP:\n","    def __init__(self, layer_dims, activation_funcs, learning_rate=0.01, epochs=1000):\n","        \"\"\"\n","        Inicializa o Multi Layer Perceptron.\n","\n","        Args:\n","            layer_dims (list): Uma lista de inteiros\n","            activation_funcs (list): Uma lista de strings com os nomes das funções de ativação\n","                                     para cada camada oculta e a camada de saída.\n","                                     O tamanho deve ser layer_dims - 1\n","            learning_rate (float): A taxa de aprendizado para o otimizador.\n","            epochs (int): O número de épocas de treinamento.\n","        \"\"\"\n","        self.layer_dims = layer_dims\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.weights = []\n","        self.biases = []\n","        self.activation_funcs = []\n","\n","        # Validação das funções de ativação\n","        if len(activation_funcs) != len(layer_dims) - 1:\n","            raise ValueError(\"O número de funções de ativação deve ser igual ao número de camadas - 1.\")\n","\n","        for af_name in activation_funcs:\n","            if af_name not in ACTIVATION_FUNCTIONS:\n","                raise ValueError(f\"Função de ativação '{af_name}' não suportada.\")\n","            self.activation_funcs.append(ACTIVATION_FUNCTIONS[af_name])\n","\n","        # Inicializa pesos e bias\n","        for i in range(len(self.layer_dims) - 1):\n","            weight_matrix = np.random.randn(self.layer_dims[i], self.layer_dims[i+1]) * 0.01\n","            bias_vector = np.zeros((1, self.layer_dims[i+1]))\n","            self.weights.append(weight_matrix)\n","            self.biases.append(bias_vector)\n","\n","    def _forward_pass(self, X):\n","        \"\"\"\n","        Realiza a propagação direta (forward pass).\n","        Retorna as ativações de todas as camadas para uso no backpropagation.\n","        \"\"\"\n","        activations = [X]\n","        current_activation = X\n","        for i in range(len(self.weights)):\n","            # Z = WX + B\n","            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n","            # A = g(Z)\n","            current_activation = self.activation_funcs[i]['func'](z)\n","            activations.append(current_activation)\n","        return activations\n","\n","    def _backward_pass(self, X, y, activations):\n","        \"\"\"\n","        Realiza a propagação reversa (backward pass) e atualiza pesos e bias.\n","        \"\"\"\n","        num_layers = len(self.layer_dims)\n","        deltas = [None] * num_layers\n","        d_weights = [None] * (num_layers - 1)\n","        d_biases = [None] * (num_layers - 1)\n","\n","        # Erro na camada de saída\n","        # dE/dA_L = A_L - Y (para MSE)\n","        error_output = activations[-1] - y\n","\n","        # Delta da camada de saída\n","        # delta_L = dE/dA_L * g'(Z_L)\n","        deltas[-1] = error_output * self.activation_funcs[-1]['derivative'](activations[-1])\n","\n","        # Propagação do erro para trás\n","        for i in reversed(range(num_layers - 1)):\n","            # dW = A_{L-1}^T * delta_L\n","            d_weights[i] = np.dot(activations[i].T, deltas[i+1])\n","            # db = sum(delta_L)\n","            d_biases[i] = np.sum(deltas[i+1], axis=0, keepdims=True)\n","\n","            if i > 0: # Não calcula delta para a camada de entrada\n","                # delta_L-1 = delta_L * W_L^T * g'(Z_L-1)\n","                deltas[i] = np.dot(deltas[i+1], self.weights[i].T) * self.activation_funcs[i-1]['derivative'](activations[i])\n","\n","        # Atualiza pesos e bias\n","        for i in range(num_layers - 1):\n","            self.weights[i] -= self.learning_rate * d_weights[i]\n","            self.biases[i] -= self.learning_rate * d_biases[i]\n","\n","    def train(self, X, y):\n","        \"\"\"\n","        Treina o MLP usando o algoritmo de backpropagation.\n","\n","        Args:\n","            X (np.array): Dados de entrada de treinamento.\n","            y (np.array): Rótulos verdadeiros.\n","        \"\"\"\n","        for epoch in range(self.epochs):\n","            activations = self._forward_pass(X)\n","            self._backward_pass(X, y, activations)\n","\n","            if epoch % (self.epochs // 10) == 0:\n","                loss = np.mean(np.square(y - activations[-1])) # Mean Squared Error\n","                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss:.4f}\")\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Faz previsões com o MLP treinado.\n","        \"\"\"\n","        activations = self._forward_pass(X)\n","        # Para classificação, geralmente usamos um limiar de 0.5 para sigmoid\n","        # ou argmax para softmax (se você adicionar softmax na saída)\n","        return (activations[-1] > 0.5).astype(int) # para classificação binária com sigmoid na saída"],"metadata":{"id":"JCsNfzXD0agY","executionInfo":{"status":"ok","timestamp":1752259983289,"user_tz":180,"elapsed":46,"user":{"displayName":"Ecv Asl","userId":"09710885291478391939"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Teste e Comparação"],"metadata":{"id":"BS4QI1iq1A9u"}},{"cell_type":"code","source":["# Gerar dados de exemplo\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n","                           n_redundant=5, n_classes=2, random_state=42)\n","y = y.reshape(-1, 1) # Scikit-learn espera 1D, mas nossa implementação espera 2D\n","\n","# Dividir em conjuntos de treinamento e teste\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(f\"Forma dos dados de treinamento X: {X_train.shape}, y: {y_train.shape}\")\n","print(f\"Forma dos dados de teste X: {X_test.shape}, y: {y_test.shape}\")\n","\n","# --- Teste com a sua implementação ---\n","print(\"\\n--- Treinando meu MLP ---\")\n","# Exemplo: 20 -> 10 -> 5 -> 1 (entrada -> oculta1 -> oculta2 -> saída)\n","# Funções de ativação para as camadas ocultas e de saída\n","my_mlp = MyMLP(layer_dims=[X_train.shape[1], 10, 5, y_train.shape[1]],\n","               activation_funcs=['relu', 'tanh', 'sigmoid'], # relu para 10, tanh para 5, sigmoid para 1\n","               learning_rate=0.01,\n","               epochs=2000)\n","my_mlp.train(X_train, y_train)\n","\n","y_pred_my_mlp = my_mlp.predict(X_test)\n","accuracy_my_mlp = accuracy_score(y_test, y_pred_my_mlp)\n","print(f\"\\nAcurácia do meu MLP: {accuracy_my_mlp:.4f}\")\n","\n","# --- Teste com Scikit-learn ---\n","print(\"\\n--- Treinando MLP do Scikit-learn ---\")\n","# Scikit-learn espera y como 1D para classificação binária\n","y_train_sklearn = y_train.flatten()\n","y_test_sklearn = y_test.flatten()\n","\n","# Note que a configuração do Scikit-learn é um pouco diferente\n","# hidden_layer_sizes define as camadas ocultas.\n","# 'logistic' é sigmoid, 'relu' é ReLU, 'tanh' é tanh.\n","mlp_sklearn = MLPClassifier(hidden_layer_sizes=(10, 5), # Duas camadas ocultas com 10 e 5 neurônios\n","                            activation='relu', # Ativação para as camadas ocultas\n","                            solver='adam', # Otimizador Adam, mais robusto que SGD simples\n","                            alpha=0.0001, # Regularização L2\n","                            batch_size='auto',\n","                            learning_rate_init=0.01,\n","                            max_iter=2000,\n","                            random_state=42,\n","                            verbose=False) # Mude para True para ver o treinamento\n","\n","mlp_sklearn.fit(X_train, y_train_sklearn)\n","\n","y_pred_sklearn = mlp_sklearn.predict(X_test)\n","accuracy_sklearn = accuracy_score(y_test_sklearn, y_pred_sklearn)\n","print(f\"Acurácia do MLP do Scikit-learn: {accuracy_sklearn:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw4DnDA91EyW","executionInfo":{"status":"ok","timestamp":1752259986772,"user_tz":180,"elapsed":3481,"user":{"displayName":"Ecv Asl","userId":"09710885291478391939"}},"outputId":"bf5fa048-45b2-4e15-b22f-5701aaafab12"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma dos dados de treinamento X: (800, 20), y: (800, 1)\n","Forma dos dados de teste X: (200, 20), y: (200, 1)\n","\n","--- Treinando meu MLP ---\n","Epoch 0/2000, Loss: 0.2500\n","Epoch 200/2000, Loss: 0.0190\n","Epoch 400/2000, Loss: 0.0090\n","Epoch 600/2000, Loss: 0.0088\n","Epoch 800/2000, Loss: 0.0088\n","Epoch 1000/2000, Loss: 0.0087\n","Epoch 1200/2000, Loss: 0.0075\n","Epoch 1400/2000, Loss: 0.0075\n","Epoch 1600/2000, Loss: 0.0075\n","Epoch 1800/2000, Loss: 0.0075\n","\n","Acurácia do meu MLP: 0.9200\n","\n","--- Treinando MLP do Scikit-learn ---\n","Acurácia do MLP do Scikit-learn: 0.9150\n"]}]}]}